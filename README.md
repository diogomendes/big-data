# Big Data

This project was a learning experience on how to handle large datasets that cannot be processed with conventional tools or would take a significant amount of time to execute. Various techniques were explored, including pipelines, GPUs, Rapids-AI, multiprocessing, and BigQuery from Google Cloud Platform (GCP).

The folder contains 3 files:

Primary Data Analysis -> This file contains data analysis charts to help us better understand the tables and identify important columns.

Table Preparation -> This file includes the selection of important columns from all tables and the preparation of 3 different datasets for later model building. A model was also created using BigQuery ML in this file.

Data Preparation and Models -> This file was executed in an environment with Rapids-AI on Ubuntu, utilizing methods such as GPU, multiprocessing, and pipelines.
